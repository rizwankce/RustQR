name: Full Benchmark

on:
  workflow_dispatch:
    inputs:
      bench_limit:
        description: 'Number of images to benchmark (0 = all 536)'
        required: false
        default: '0'
        type: string
      run_reading_rate:
        description: 'Run reading rate benchmark'
        required: false
        default: true
        type: boolean
      run_criterion:
        description: 'Run Criterion benchmarks'
        required: false
        default: true
        type: boolean
      baseline_artifact:
        description: 'Optional baseline artifact JSON path for regression gates'
        required: false
        default: ''
        type: string
      max_rate_drop_pp:
        description: 'Max allowed weighted-global drop (percentage points)'
        required: false
        default: '1.0'
        type: string
      max_median_runtime_regression_pct:
        description: 'Max allowed median runtime regression (%)'
        required: false
        default: '15.0'
        type: string
      category_gates:
        description: 'Per-category max drops (comma-separated NAME=PP)'
        required: false
        default: 'lots=2.0,rotations=2.0,nominal=1.5,high_version=1.5'
        type: string

env:
  CARGO_TERM_COLOR: always
  QR_MAX_DIM: "1024"

jobs:
  benchmark:
    name: Benchmark on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        include:
          - os: ubuntu-latest
            platform: linux
          - os: macos-latest
            platform: macos
          - os: windows-latest
            platform: windows

    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-bench-
            ${{ runner.os }}-cargo-

      - name: Build release (with tools)
        run: cargo build --release --features tools

      - name: Run reading rate benchmark
        if: ${{ inputs.run_reading_rate }}
        env:
          QR_BENCH_LIMIT: ${{ inputs.bench_limit }}
        run: |
          echo "## Reading Rate Results - ${{ matrix.platform }}" >> $GITHUB_STEP_SUMMARY
          echo "Command: \`cargo run --features tools --bin qrtool --release -- reading-rate --non-interactive --artifact-json reading_rate_${{ matrix.platform }}.json\`" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cargo run --features tools --bin qrtool --release -- \
            reading-rate \
            --non-interactive \
            --artifact-json "reading_rate_${{ matrix.platform }}.json" \
            2>&1 | tee "reading_rate_${{ matrix.platform }}.txt"
          cat reading_rate_${{ matrix.platform }}.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
        shell: bash

      - name: Run Criterion benchmarks (smoke)
        if: ${{ inputs.run_criterion }}
        env:
          QR_SMOKE: "1"
        run: |
          echo "## Criterion Smoke Test - ${{ matrix.platform }}" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cargo bench --features tools --bench real_qr_images 2>&1 | tee criterion_smoke_${{ matrix.platform }}.txt
          cat criterion_smoke_${{ matrix.platform }}.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
        shell: bash

      - name: Run Criterion benchmarks (full)
        if: ${{ inputs.run_criterion }}
        env:
          QR_BENCH_LIMIT: ${{ inputs.bench_limit }}
        run: |
          echo "## Criterion Full Test (${{ inputs.bench_limit }} images) - ${{ matrix.platform }}" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cargo bench --features tools --bench real_qr_images 2>&1 | tee criterion_full_${{ matrix.platform }}.txt
          cat criterion_full_${{ matrix.platform }}.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
        shell: bash

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.platform }}
          path: |
            reading_rate_*.txt
            reading_rate_*.json
            reading_rate_contribution_*.json
            criterion_*.txt
            target/criterion/
          retention-days: 30

      - name: Regression gate (macOS reading-rate)
        if: ${{ inputs.run_reading_rate && matrix.platform == 'macos' && inputs.baseline_artifact != '' }}
        run: |
          IFS=',' read -ra CATEGORY_GATES <<< "${{ inputs.category_gates }}"
          CATEGORY_ARGS=()
          for gate in "${CATEGORY_GATES[@]}"; do
            gate_trimmed="$(echo "$gate" | xargs)"
            if [ -n "$gate_trimmed" ]; then
              CATEGORY_ARGS+=("--category-max-drop-pp" "$gate_trimmed")
            fi
          done
          python3 scripts/compare_reading_rate_artifacts.py \
            --baseline "${{ inputs.baseline_artifact }}" \
            --candidate "reading_rate_${{ matrix.platform }}.json" \
            --max-rate-drop-pp "${{ inputs.max_rate_drop_pp }}" \
            --max-median-runtime-regression-pct "${{ inputs.max_median_runtime_regression_pct }}" \
            --contribution-report "reading_rate_contribution_${{ matrix.platform }}.json" \
            "${CATEGORY_ARGS[@]}"
        shell: bash

  summary:
    name: Aggregate Results
    needs: benchmark
    runs-on: ubuntu-latest
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: results

      - name: Create summary
        run: |
          echo "# Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Reading Rate Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          for platform in linux macos windows; do
            if [ -f "results/benchmark-results-$platform/reading_rate_$platform.txt" ]; then
              echo "### $platform" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              grep -E "(Rate:|total|===|/[0-9])" "results/benchmark-results-$platform/reading_rate_$platform.txt" >> $GITHUB_STEP_SUMMARY || true
              echo '```' >> $GITHUB_STEP_SUMMARY
            fi
          done

          if [ -f "results/benchmark-results-linux/reading_rate_linux.json" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Artifact Compare Command" >> $GITHUB_STEP_SUMMARY
            echo "\`python3 scripts/compare_reading_rate_artifacts.py --baseline <baseline.json> --candidate results/benchmark-results-linux/reading_rate_linux.json\`" >> $GITHUB_STEP_SUMMARY
          fi
